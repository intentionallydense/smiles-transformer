run 1: lr = 3e-3

batch 0: loss = 7.0561
batch 100: loss = 0.5903
batch 200: loss = 0.4760
batch 300: loss = 0.4137
batch 400: loss = 0.4747
batch 500: loss = 0.4960
batch 600: loss = 0.4769
batch 700: loss = 0.4986
batch 800: loss = 0.5524
batch 900: loss = 0.4731
batch 1000: loss = 0.5335
batch 1100: loss = 0.4377
batch 1200: loss = 0.5221
batch 1300: loss = 0.3730
batch 1400: loss = 0.4999
batch 1500: loss = 0.4823
batch 1600: loss = 0.4462
Epoch 1: train loss = 0.5201
Epoch: 1, training loss: 0.5201, validation loss: 0.4455
new best average validation loss! 0.4455 < 0.4455
batch 0: loss = 0.4791
batch 100: loss = 0.4350
batch 200: loss = 0.4387
batch 300: loss = 0.4370
batch 400: loss = 0.4364
batch 500: loss = 0.3545
batch 600: loss = 0.4072
batch 700: loss = 0.4256
batch 800: loss = 0.4117
batch 900: loss = 0.4698
batch 1000: loss = 0.3646
batch 1100: loss = 0.5070
batch 1200: loss = 0.4457
batch 1300: loss = 0.5139
batch 1400: loss = 0.4551
batch 1500: loss = 0.3936
batch 1600: loss = 0.4605
Epoch 2: train loss = 0.4399
Epoch: 2, training loss: 0.4399, validation loss: 0.4293
new best average validation loss! 0.4293 < 0.4293
batch 0: loss = 0.4078
batch 100: loss = 0.4597
batch 200: loss = 0.4815
batch 300: loss = 0.4674
batch 400: loss = 0.3996
batch 500: loss = 0.3970
batch 600: loss = 0.4711
batch 700: loss = 0.4087
batch 800: loss = 0.3723
batch 900: loss = 0.3865
batch 1000: loss = 0.4049
batch 1100: loss = 0.4488
batch 1200: loss = 0.4047
batch 1300: loss = 0.5491
batch 1400: loss = 0.5079
batch 1500: loss = 0.4813
batch 1600: loss = 0.4303
Epoch 3: train loss = 0.4341
Epoch: 3, training loss: 0.4341, validation loss: 0.4352
batch 0: loss = 0.3975
batch 100: loss = 0.4583
batch 200: loss = 0.4588
batch 300: loss = 0.4895
batch 400: loss = 0.3924
batch 500: loss = 0.4340
batch 600: loss = 0.4685
batch 700: loss = 0.3884
batch 800: loss = 0.4045
batch 900: loss = 0.5187
batch 1000: loss = 0.4815
batch 1100: loss = 0.4678
batch 1200: loss = 0.4589
batch 1300: loss = 0.5015
batch 1400: loss = 0.3758
batch 1500: loss = 0.3795
batch 1600: loss = 0.3987
Epoch 4: train loss = 0.4306
Epoch: 4, training loss: 0.4306, validation loss: 0.4218
new best average validation loss! 0.4218 < 0.4218
batch 0: loss = 0.3745
batch 100: loss = 0.4102
batch 200: loss = 0.3928
batch 300: loss = 0.4354
batch 400: loss = 0.3839
batch 500: loss = 0.3976
batch 600: loss = 0.3953
batch 700: loss = 0.4824
batch 800: loss = 0.4250
batch 900: loss = 0.4480
batch 1000: loss = 0.4501
batch 1100: loss = 0.4079
batch 1200: loss = 0.3596
batch 1300: loss = 0.4866
batch 1400: loss = 0.3218
batch 1500: loss = 0.3682
batch 1600: loss = 0.4227
Epoch 5: train loss = 0.4229
Epoch: 5, training loss: 0.4229, validation loss: 0.4196
new best average validation loss! 0.4196 < 0.4196
batch 0: loss = 0.4628
batch 100: loss = 0.4149
batch 200: loss = 0.3831
batch 300: loss = 0.3955
batch 400: loss = 0.4338
batch 500: loss = 0.3373
batch 600: loss = 0.4603
batch 700: loss = 0.4358
batch 800: loss = 0.4355
batch 900: loss = 0.3647
batch 1000: loss = 0.4828
batch 1100: loss = 0.3298
batch 1200: loss = 0.3901
batch 1300: loss = 0.4575
batch 1400: loss = 0.4817
batch 1500: loss = 0.4225
batch 1600: loss = 0.3920
Epoch 6: train loss = 0.4211
Epoch: 6, training loss: 0.4211, validation loss: 0.4184
new best average validation loss! 0.4184 < 0.4184
batch 0: loss = 0.4164
batch 100: loss = 0.4457
batch 200: loss = 0.2600
batch 300: loss = 0.3930
batch 400: loss = 0.4491
batch 500: loss = 0.4077
batch 600: loss = 0.3136
batch 700: loss = 0.4638
batch 800: loss = 0.3295
batch 900: loss = 0.4780
batch 1000: loss = 0.4082
batch 1100: loss = 0.4170
batch 1200: loss = 0.4185
batch 1300: loss = 0.4261
batch 1400: loss = 0.3467
batch 1500: loss = 0.4689
batch 1600: loss = 0.5523
Epoch 7: train loss = 0.4142
Epoch: 7, training loss: 0.4142, validation loss: 0.4240
batch 0: loss = 0.5153
batch 100: loss = 0.4016
batch 200: loss = 0.3675
batch 300: loss = 0.3780
batch 400: loss = 0.5272
batch 500: loss = 0.3491
batch 600: loss = 0.4275
batch 700: loss = 0.4088
batch 800: loss = 0.4108
batch 900: loss = 0.3741
batch 1000: loss = 0.3651
batch 1100: loss = 0.3932
batch 1200: loss = 0.4149
batch 1300: loss = 0.4629
batch 1400: loss = 0.3824
batch 1500: loss = 0.4385
batch 1600: loss = 0.4227
Epoch 8: train loss = 0.4162
Epoch: 8, training loss: 0.4162, validation loss: 0.4088
new best average validation loss! 0.4088 < 0.4088
batch 0: loss = 0.4270
batch 100: loss = 0.4157
batch 200: loss = 0.4631
batch 300: loss = 0.4915
batch 400: loss = 0.4431
batch 500: loss = 0.3779
batch 600: loss = 0.4658
batch 700: loss = 0.4795
batch 800: loss = 0.4139
batch 900: loss = 0.4214
batch 1000: loss = 0.4242
batch 1100: loss = 0.4724
batch 1200: loss = 0.4135
batch 1300: loss = 0.4994
batch 1400: loss = 0.3829
batch 1500: loss = 0.4479
batch 1600: loss = 0.4132
Epoch 9: train loss = 0.4171
Epoch: 9, training loss: 0.4171, validation loss: 0.4196
batch 0: loss = 0.4435
batch 100: loss = 0.3877
batch 200: loss = 0.3866
batch 300: loss = 0.4570
batch 400: loss = 0.5119
batch 500: loss = 0.4661
batch 600: loss = 0.3445
batch 700: loss = 0.4031
batch 800: loss = 0.4471
batch 900: loss = 0.4743
batch 1000: loss = 0.4710
batch 1100: loss = 0.4887
batch 1200: loss = 0.3599
batch 1300: loss = 0.4330
batch 1400: loss = 0.3673
batch 1500: loss = 0.3875
batch 1600: loss = 0.3988
Epoch 10: train loss = 0.4152
Epoch: 10, training loss: 0.4152, validation loss: 0.4217
batch 0: loss = 0.3934
batch 100: loss = 0.4171
batch 200: loss = 0.3607
batch 300: loss = 0.3524
batch 400: loss = 0.4891
batch 500: loss = 0.3586
batch 600: loss = 0.4397
batch 700: loss = 0.3754
batch 800: loss = 0.3517
batch 900: loss = 0.4517
batch 1000: loss = 0.4027
batch 1100: loss = 0.4796
batch 1200: loss = 0.3944
batch 1300: loss = 0.4301
batch 1400: loss = 0.4606
batch 1500: loss = 0.3976
batch 1600: loss = 0.4445
Epoch 11: train loss = 0.4221
Epoch: 11, training loss: 0.4221, validation loss: 0.4241
batch 0: loss = 0.3968
batch 100: loss = 0.4708
batch 200: loss = 0.4729
batch 300: loss = 0.4100
batch 400: loss = 0.4466
batch 500: loss = 0.4355
batch 600: loss = 0.3924
batch 700: loss = 0.3773
batch 800: loss = 0.3968
batch 900: loss = 0.3399
batch 1000: loss = 0.3493
batch 1100: loss = 0.3549
batch 1200: loss = 0.3775
batch 1300: loss = 0.3414
batch 1400: loss = 0.3805
batch 1500: loss = 0.4325
batch 1600: loss = 0.4800
Epoch 12: train loss = 0.4139
Epoch: 12, training loss: 0.4139, validation loss: 0.4051
new best average validation loss! 0.4051 < 0.4051
batch 0: loss = 0.4124
batch 100: loss = 0.4113
batch 200: loss = 0.4039
batch 300: loss = 0.4365
batch 400: loss = 0.3746
batch 500: loss = 0.4017
batch 600: loss = 0.4567
batch 700: loss = 0.4338
batch 800: loss = 0.4664
batch 900: loss = 0.4485
batch 1000: loss = 0.4923
batch 1100: loss = 0.4035
batch 1200: loss = 0.3848
batch 1300: loss = 0.4598
batch 1400: loss = 0.3837
batch 1500: loss = 0.3791
batch 1600: loss = 0.4674
Epoch 13: train loss = 0.4081
Epoch: 13, training loss: 0.4081, validation loss: 0.4052
batch 0: loss = 0.4581
batch 100: loss = 0.3704
batch 200: loss = 0.3368
batch 300: loss = 0.4158
batch 400: loss = 0.4649
batch 500: loss = 0.3817
batch 600: loss = 0.4841
batch 700: loss = 0.3899
batch 800: loss = 0.4729
batch 900: loss = 0.4527
batch 1000: loss = 0.4113
batch 1100: loss = 0.3311
batch 1200: loss = 0.4231
batch 1300: loss = 0.4057
batch 1400: loss = 0.4250
batch 1500: loss = 0.4290
batch 1600: loss = 0.3309
Epoch 14: train loss = 0.4046
Epoch: 14, training loss: 0.4046, validation loss: 0.4011
new best average validation loss! 0.4011 < 0.4011
batch 0: loss = 0.4254
batch 100: loss = 0.3681
batch 200: loss = 0.3171
batch 300: loss = 0.4177
batch 400: loss = 0.3483
batch 500: loss = 0.4375
batch 600: loss = 0.3670
batch 700: loss = 0.3545
batch 800: loss = 0.3485
batch 900: loss = 0.3656
batch 1000: loss = 0.3707
batch 1100: loss = 0.3837
batch 1200: loss = 0.3208
batch 1300: loss = 0.4171
batch 1400: loss = 0.4681
batch 1500: loss = 0.4215
batch 1600: loss = 0.4714
Epoch 15: train loss = 0.4016
Epoch: 15, training loss: 0.4016, validation loss: 0.4129
batch 0: loss = 0.4513
batch 100: loss = 0.4516
batch 200: loss = 0.3438
batch 300: loss = 0.4770
batch 400: loss = 0.4526
batch 500: loss = 0.3715
batch 600: loss = 0.4008
batch 700: loss = 0.4316
batch 800: loss = 0.4493
batch 900: loss = 0.4293
batch 1000: loss = 0.4606
batch 1100: loss = 0.4103
batch 1200: loss = 0.3779
batch 1300: loss = 0.3818
batch 1400: loss = 0.4648
batch 1500: loss = 0.3951
batch 1600: loss = 0.4390
Epoch 16: train loss = 0.4039
Epoch: 16, training loss: 0.4039, validation loss: 0.4034
batch 0: loss = 0.4089
batch 100: loss = 0.3979
batch 200: loss = 0.3840
batch 300: loss = 0.4022
batch 400: loss = 0.3232
batch 500: loss = 0.3995
batch 600: loss = 0.3406
batch 700: loss = 0.4156
batch 800: loss = 0.4239
batch 900: loss = 0.3607
batch 1000: loss = 0.4064
batch 1100: loss = 0.4606
batch 1200: loss = 0.5301
batch 1300: loss = 0.3832
batch 1400: loss = 0.4702
batch 1500: loss = 0.4192
batch 1600: loss = 0.5073
Epoch 17: train loss = 0.4055
Epoch: 17, training loss: 0.4055, validation loss: 0.4122
batch 0: loss = 0.4302
batch 100: loss = 0.3787
batch 200: loss = 0.3679
batch 300: loss = 0.4636
batch 400: loss = 0.4339
batch 500: loss = 0.4240
batch 600: loss = 0.4028
batch 700: loss = 0.3765
batch 800: loss = 0.3955
batch 900: loss = 0.3794
batch 1000: loss = 0.4825
batch 1100: loss = 0.4532
batch 1200: loss = 0.3728
batch 1300: loss = 0.4188
batch 1400: loss = 0.3347
batch 1500: loss = 0.4264
batch 1600: loss = 0.3613
Epoch 18: train loss = 0.4053
Epoch: 18, training loss: 0.4053, validation loss: 0.4031
batch 0: loss = 0.4177
batch 100: loss = 0.3939
batch 200: loss = 0.2909
batch 300: loss = 0.4432
batch 400: loss = 0.4364
batch 500: loss = 0.3659
batch 600: loss = 0.4710
batch 700: loss = 0.3928
batch 800: loss = 0.3744
batch 900: loss = 0.3987
batch 1000: loss = 0.4164
batch 1100: loss = 0.4673
batch 1200: loss = 0.4241
batch 1300: loss = 0.3930
batch 1400: loss = 0.3664
batch 1500: loss = 0.3947
batch 1600: loss = 0.3996
Epoch 19: train loss = 0.4005
Epoch: 19, training loss: 0.4005, validation loss: 0.4097
batch 0: loss = 0.3141
batch 100: loss = 0.3314
batch 200: loss = 0.3358
batch 300: loss = 0.4183
batch 400: loss = 0.3462
batch 500: loss = 0.4439
batch 600: loss = 0.4692
batch 700: loss = 0.4640
batch 800: loss = 0.3670
batch 900: loss = 0.4225
batch 1000: loss = 0.3942
batch 1100: loss = 0.3726
batch 1200: loss = 0.4191
batch 1300: loss = 0.4139
batch 1400: loss = 0.3436
batch 1500: loss = 0.3576
batch 1600: loss = 0.4072
Epoch 20: train loss = 0.3987
Epoch: 20, training loss: 0.3987, validation loss: 0.3936
new best average validation loss! 0.3936 < 0.3936